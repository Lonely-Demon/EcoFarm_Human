# Monitoring & Alerting for EcoFarm_Human MVP

This document explains how to configure monitoring, health checks, and alerting for the MVP services using Render and Sentry (recommended). It also outlines common practices for service health and logs.

## Overview
- Each service exposes a `/healthz` endpoint returning `{ "status": "ok" }` for readiness checks.
- Render provides logs & basic health checks; Sentry provides error aggregation and alerts.
- Use Render's health checks (HTTP) to restart unhealthy containers.

## Sentry Setup (recommended)
1. Create a Sentry project at https://sentry.io/ and copy the DSN.
2. Add `SENTRY_DSN` to each service’s Render environment variables.
3. Sentry automatically collects exceptions, stack traces, and performance traces. Use Sentry rules to configure email/Slack alerts for production/critical errors.

## Render health checks
- Configure a Render **Health Check** for each service using the `http://<service>.onrender.com/healthz` endpoint.
- Set a `Response code` of `200` and `Timeout` of `5` seconds.
- Configure `Restart` on `Unhealthy` to auto-restore unhealthy instances.

## Alarm & Notification Strategy
- Configure Sentry alerts for:
  - High error rate (e.g., >10% 5xx errors per minute)
  - Repeated exceptions within a small time window
  - Performance degradation (response time > 2s)
- Configure Render alerts to notify on deploy failures and high CPU/memory.

## Logs & Aggregation
- Render provides real-time logs; use Sentry for application-level errors & traces.
- For DB & application-level queries, track slow queries (Supabase) and monitor query performance.

## Health Check & Readiness Probes
- For services requiring DB access, implement a lightweight DB ping in `healthz` to return DB status. For now, simple `status: ok` is sufficient for MVP; we can iterate later.

## Incident Response
1. For immediate failures, view the service logs in Render and check Sentry events.
2. If the failure is due to DB or configuration (e.g., rotated keys), mark the affected service as `paused` and update environment variables.
3. Always roll back to previous stable release if deploy is the cause; use Render's `Rollback` button.

## Maintenance & Cost Control
- Use Sentry's alerts & rate limiting to control events generated by noisy exceptions.
- Set daily cost alerts for Hugging Face or inference providers; monitor calls & amortize by caching.

## Next steps for enhanced monitoring (Phase 2)
- Add cross-service tracing (e.g., Sentry traces or OpenTelemetry + Jaeger) for end-to-end requests.
- Add per-service dashboards (Datadog/Prometheus + Grafana) for metrics (request count, latency, error rate, CPU/memory). 
- Add a synthetic check runner (GitHub Actions scheduled) to call key endpoints daily and post results to a dashboard or Slack.

## Synthetic Checks (Scheduled)

We added a GitHub Actions workflow (`.github/workflows/synthetic-checks.yml`) that runs every 6 hours to check staging (`STAGING_BASE_URL`) and production (`PROD_BASE_URL`) endpoints.

What it checks:
- `GET /healthz` — root health readiness
- `GET /weather?lat=12.9&lon=80.2` — weather endpoint
- `POST /ai/rec/` — AI recommendations (sample POST with lat/lon)

Configuration:
- Add `STAGING_BASE_URL` and `PROD_BASE_URL` GitHub repo secrets (e.g., `https://staging-ecofarm.onrender.com`).
- Add optional `SLACK_WEBHOOK` secret if you want the workflow to post a short notification to Slack on both success or failure.

Behavior:
- The job fails if any endpoint returns a non-200 response.
- Logs include the status and response for debugging.
- On repeated failures, check the Render service logs and Sentry events for errors.

Next steps:
- Add automatic issue creation or page alerts if a check fails repeatedly (requires extra permissions & a Slack or PagerDuty integration).
